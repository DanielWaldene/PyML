{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "torch.random.manual_seed(1)\n",
    "\n",
    "n_classes = 4 #number of classification catetory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make transforms and use data loaders\n",
    "\n",
    "mean_nums = [0.485, 0.456, 0.406] #use these two as fixed number\n",
    "std_nums = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transforms = transforms.Compose([   #apply these 4 preprcoessing steps\n",
    "        transforms.Resize(256), #resizing image\n",
    "        transforms.CenterCrop(224), #crop image to 224*224\n",
    "        transforms.ToTensor(), #covert to tensor \n",
    "        transforms.Normalize(mean_nums, std_nums) # refers to RGB values; \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory for the data\n",
    "data_dir = 'Data/Bearing_Images'\n",
    "\n",
    "# Use the image folder function to create datasets; use sub-folder namers to annotate the images within subfolders\n",
    "train_data = datasets.ImageFolder(data_dir,transform=data_transforms)\n",
    "\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.3\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data) #get total number of data: 240\n",
    "indices = list(range(num_train)) #create a index: 0-239\n",
    "np.random.shuffle(indices) #shuffle index\n",
    "valid_split = int(np.floor((valid_size) * num_train)) #generate a breakpoint to split training and testing: 72\n",
    "valid_idx, train_idx = indices[:valid_split], indices[valid_split:] # first 72 samples are testing samples; remaining are training samples\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx) #convert list index to be recognizable by torch\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler), batch_size refers to batch training, how many samples to be trained at one iteration\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=20,\n",
    "    sampler=train_sampler) #train_loader contain not only the images but also the labels\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=20, \n",
    "    sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, padding =1, stride=4), #(224-11+2)/4+1 =54 (20,3,224,224)->(20,96,54,54)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)) #54-3/2 +1=26           (20,96,26,26)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1), #26-5/1 +1=22 (20,256,22,22)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)) #22-3/2 +1=10 (20,256,10,10)\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding =1, stride=1), #10+2-3/1 +1=10 (20,384,10,10)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)) #10-3/2 +1=4  (20,384,4,4)\n",
    "\n",
    "        self.drop_out = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(6144, 100) #(20,384,4,4) -> (20, 6144) -> (20,100)\n",
    "        self.fc2 = nn.Linear(100, 4)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) #if you remove this line, use CrossEntropyLoss\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1) #-1 means the system will do an automatic calculation of 384*4*4\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out) #if you remove this line, use CrossEntropyLoss\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.619236 \tValidation Loss: 0.706025 \tValidation Accuracy: 0.591667\n",
      "Epoch: 2 \tTraining Loss: 0.563844 \tValidation Loss: 0.434780 \tValidation Accuracy: 0.783333\n",
      "Epoch: 3 \tTraining Loss: 0.446282 \tValidation Loss: 0.449098 \tValidation Accuracy: 0.670833\n",
      "Epoch: 4 \tTraining Loss: 0.388446 \tValidation Loss: 0.384137 \tValidation Accuracy: 0.720833\n",
      "Epoch: 5 \tTraining Loss: 0.422829 \tValidation Loss: 0.436953 \tValidation Accuracy: 0.708333\n",
      "Epoch: 6 \tTraining Loss: 0.386051 \tValidation Loss: 0.356606 \tValidation Accuracy: 0.791667\n",
      "Epoch: 7 \tTraining Loss: 0.372384 \tValidation Loss: 0.361097 \tValidation Accuracy: 0.750000\n",
      "Epoch: 8 \tTraining Loss: 0.359063 \tValidation Loss: 0.380679 \tValidation Accuracy: 0.695833\n",
      "Epoch: 9 \tTraining Loss: 0.279699 \tValidation Loss: 0.265257 \tValidation Accuracy: 0.804167\n",
      "Epoch: 10 \tTraining Loss: 0.461528 \tValidation Loss: 1.041569 \tValidation Accuracy: 0.929167\n",
      "Epoch: 11 \tTraining Loss: 0.916461 \tValidation Loss: 0.169351 \tValidation Accuracy: 0.929167\n",
      "Epoch: 12 \tTraining Loss: 0.440812 \tValidation Loss: 0.271060 \tValidation Accuracy: 0.975000\n",
      "Epoch: 13 \tTraining Loss: 0.570158 \tValidation Loss: 0.033637 \tValidation Accuracy: 1.000000\n",
      "Epoch: 14 \tTraining Loss: 0.032073 \tValidation Loss: 1.085292 \tValidation Accuracy: 0.950000\n",
      "Epoch: 15 \tTraining Loss: 0.316204 \tValidation Loss: 0.023333 \tValidation Accuracy: 1.000000\n",
      "Epoch: 16 \tTraining Loss: 0.022580 \tValidation Loss: 0.006780 \tValidation Accuracy: 1.000000\n",
      "Epoch: 17 \tTraining Loss: 0.004321 \tValidation Loss: 0.003342 \tValidation Accuracy: 1.000000\n",
      "Epoch: 18 \tTraining Loss: 0.002121 \tValidation Loss: 0.000824 \tValidation Accuracy: 1.000000\n",
      "Epoch: 19 \tTraining Loss: 0.001231 \tValidation Loss: 0.000624 \tValidation Accuracy: 1.000000\n",
      "Epoch: 20 \tTraining Loss: 0.000680 \tValidation Loss: 0.000268 \tValidation Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "epochs = 20 #difference between epoch and iteration: if don't apply batch training, they are the same; otherwise, one iteration correspond to training one batch of data, and one epoch correspond to training all data batches once.\n",
    "    \n",
    "# Loss and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "#valid_loss_min = np.Inf\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, labels in train_loader: #training\n",
    "        logps = model(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()    \n",
    "       \n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "    accuracy = 0\n",
    "    for inputs, labels in valid_loader:\n",
    "        logps = model.forward(inputs)\n",
    "        batch_loss = criterion(logps, labels)\n",
    "        valid_loss += batch_loss.item()\n",
    "        # Calculate classification accuracy\n",
    "        ps = torch.exp(logps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            \n",
    "        # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    valid_loss = valid_loss/len(valid_loader)\n",
    "    valid_accuracy = accuracy/len(valid_loader) \n",
    "      \n",
    "        # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1, train_loss, valid_loss, valid_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '2DCNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
